{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 8176199,
          "sourceType": "datasetVersion",
          "datasetId": 4839828
        }
      ],
      "dockerImageVersionId": 30699,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Assignment4_Trainer",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'amazon-fine-food:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4839828%2F8176199%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240422%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240422T121600Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D9f0d3e032c52e7d5029f3a84be9293be574eefe7588ef707b134cec7f4d31f3d990ebb4081d1e31a2c5bba7067c455498b4153249641efd85b9e90f2a459331513ddb996a08eda0261a19078bc80bd01d5f1f6b96640a3e13ddb7e9f2543e82d98dc36592ede02badf6fc097f9a472ff616223f81236321c39eeee845c5a65754fa380c54150603167015e9b162cd66e0bb5df03cba36dc47e747bbc6f430474d1ad255f4d3192f8035a2705d47bca4df81ed6c579da42eea21cba573ce0f186afaa7877003d37221ef619594888520c6054de130059fea7214d5952533c6748397e35f3ec3912a6f38d37ed21508cafb62544671e54d3b202faec5423bf9bd6'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "wHJB3SQFvBbB"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "id": "uJLm9FgQvBbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-22T11:58:25.8012Z",
          "iopub.execute_input": "2024-04-22T11:58:25.802087Z",
          "iopub.status.idle": "2024-04-22T11:58:25.807313Z",
          "shell.execute_reply.started": "2024-04-22T11:58:25.802055Z",
          "shell.execute_reply": "2024-04-22T11:58:25.80607Z"
        },
        "trusted": true,
        "id": "jk9DKXmCvBbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaning(s):\n",
        "    s = str(s)\n",
        "    s = re.sub('\\s\\W',' ',s)\n",
        "    s = re.sub('\\W,\\s',' ',s)\n",
        "    s = re.sub(\"\\d+\", \"\", s)\n",
        "    s = re.sub('\\s+',' ',s)\n",
        "    s = re.sub('[!@#$_]', '', s)\n",
        "    s = s.replace(\"co\",\"\")\n",
        "    s = s.replace(\"https\",\"\")\n",
        "    s = s.replace(\"[\\w*\",\" \")\n",
        "    return s + '\\nTL;DR:\\n'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-22T11:58:25.80956Z",
          "iopub.execute_input": "2024-04-22T11:58:25.810009Z",
          "iopub.status.idle": "2024-04-22T11:58:25.820325Z",
          "shell.execute_reply.started": "2024-04-22T11:58:25.809977Z",
          "shell.execute_reply": "2024-04-22T11:58:25.819409Z"
        },
        "trusted": true,
        "id": "I2_FjAZ9vBbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/kaggle/input/amazon-fine-food/Reviews.csv\", encoding=\"ISO-8859-1\")\n",
        "df = df.dropna(subset = ['Text', 'Summary'])\n",
        "df['Text'] = df['Text'].apply(lambda x: cleaning(x))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-22T11:58:25.82333Z",
          "iopub.execute_input": "2024-04-22T11:58:25.823619Z",
          "iopub.status.idle": "2024-04-22T11:59:26.058634Z",
          "shell.execute_reply.started": "2024-04-22T11:58:25.823595Z",
          "shell.execute_reply": "2024-04-22T11:59:26.057488Z"
        },
        "trusted": true,
        "id": "7jHi5XXevBbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-22T11:59:26.061274Z",
          "iopub.execute_input": "2024-04-22T11:59:26.062352Z",
          "iopub.status.idle": "2024-04-22T11:59:39.072099Z",
          "shell.execute_reply.started": "2024-04-22T11:59:26.062314Z",
          "shell.execute_reply": "2024-04-22T11:59:39.071004Z"
        },
        "trusted": true,
        "id": "1pQnbnQavBbF",
        "outputId": "9730f1c5-d797-4357-c8e4-035510f9dcb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import Trainer, TrainingArguments"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-22T11:59:39.075981Z",
          "iopub.execute_input": "2024-04-22T11:59:39.076312Z",
          "iopub.status.idle": "2024-04-22T11:59:39.082438Z",
          "shell.execute_reply.started": "2024-04-22T11:59:39.076282Z",
          "shell.execute_reply": "2024-04-22T11:59:39.08122Z"
        },
        "trusted": true,
        "id": "wVEMPcx_vBbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subdf = df.head(30000)\n",
        "subdf"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-22T11:59:39.083893Z",
          "iopub.execute_input": "2024-04-22T11:59:39.084931Z",
          "iopub.status.idle": "2024-04-22T11:59:39.111915Z",
          "shell.execute_reply.started": "2024-04-22T11:59:39.084903Z",
          "shell.execute_reply": "2024-04-22T11:59:39.110844Z"
        },
        "trusted": true,
        "id": "BStSza1_vBbG",
        "outputId": "7b4188d7-7df2-436b-c2ba-1e77d97cca7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 17,
          "output_type": "execute_result",
          "data": {
            "text/plain": "          Id   ProductId          UserId                      ProfileName  \\\n0          1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n1          2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n2          3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n3          4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n4          5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n...      ...         ...             ...                              ...   \n29995  29996  B000DZFMEQ   AU714FVNMGW4E       Anita L. Burnham \"Anita B\"   \n29996  29997  B000DZFMEQ  A3CZKBRQYTW7W0                     Denise Estep   \n29997  29998  B000DZFMEQ  A2LQTTTXBLFFAO                lovereading \"Gin\"   \n29998  29999  B000DZFMEQ  A2A0UL2OFEIPH4                          Jo \"Jo\"   \n29999  30000  B000DZFMEQ  A32HT263MGTCQD                       Keith Roys   \n\n       HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n0                         1                       1      5  1303862400   \n1                         0                       0      1  1346976000   \n2                         1                       1      4  1219017600   \n3                         3                       3      2  1307923200   \n4                         0                       0      5  1350777600   \n...                     ...                     ...    ...         ...   \n29995                     1                       1      5  1253750400   \n29996                     1                       1      5  1252713600   \n29997                     1                       1      5  1250208000   \n29998                     1                       1      5  1248048000   \n29999                     1                       1      5  1247184000   \n\n                                           Summary  \\\n0                            Good Quality Dog Food   \n1                                Not as Advertised   \n2                            \"Delight\" says it all   \n3                                   Cough Medicine   \n4                                      Great taffy   \n...                                            ...   \n29995                         Gluten Free Goodness   \n29996                            GLUTEN FREE BREAD   \n29997                           Pamela's bread mix   \n29998                                 Great bread!   \n29999  Great price on a great gluten-free product!   \n\n                                                    Text  \n0      I have bought several of the Vitality canned d...  \n1      Product arrived labeled as Jumbo Salted Peanut...  \n2      This is a nfection that has been around a few ...  \n3      If you are looking for the secret ingredient i...  \n4      Great taffy at a great price. There was a wide...  \n...                                                  ...  \n29995  This is the best gf bread mix I have found by ...  \n29996  THIS BREAD MIX IS THE CLOSEST THING TO REGULAR...  \n29997  Delicious and easy to make. An excellent bread...  \n29998  I bought this mix for my daughter's boyfriend,...  \n29999  We have paid over a package in health food sto...  \n\n[30000 rows x 10 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>ProductId</th>\n      <th>UserId</th>\n      <th>ProfileName</th>\n      <th>HelpfulnessNumerator</th>\n      <th>HelpfulnessDenominator</th>\n      <th>Score</th>\n      <th>Time</th>\n      <th>Summary</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>B001E4KFG0</td>\n      <td>A3SGXH7AUHU8GW</td>\n      <td>delmartian</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1303862400</td>\n      <td>Good Quality Dog Food</td>\n      <td>I have bought several of the Vitality canned d...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>B00813GRG4</td>\n      <td>A1D87F6ZCVE5NK</td>\n      <td>dll pa</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1346976000</td>\n      <td>Not as Advertised</td>\n      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>B000LQOCH0</td>\n      <td>ABXLMWJIXXAIN</td>\n      <td>Natalia Corres \"Natalia Corres\"</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1219017600</td>\n      <td>\"Delight\" says it all</td>\n      <td>This is a nfection that has been around a few ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>B000UA0QIQ</td>\n      <td>A395BORC6FGVXV</td>\n      <td>Karl</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1307923200</td>\n      <td>Cough Medicine</td>\n      <td>If you are looking for the secret ingredient i...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>B006K2ZZ7K</td>\n      <td>A1UQRSCLF8GW1T</td>\n      <td>Michael D. Bigham \"M. Wassir\"</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>1350777600</td>\n      <td>Great taffy</td>\n      <td>Great taffy at a great price. There was a wide...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>29995</th>\n      <td>29996</td>\n      <td>B000DZFMEQ</td>\n      <td>AU714FVNMGW4E</td>\n      <td>Anita L. Burnham \"Anita B\"</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1253750400</td>\n      <td>Gluten Free Goodness</td>\n      <td>This is the best gf bread mix I have found by ...</td>\n    </tr>\n    <tr>\n      <th>29996</th>\n      <td>29997</td>\n      <td>B000DZFMEQ</td>\n      <td>A3CZKBRQYTW7W0</td>\n      <td>Denise Estep</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1252713600</td>\n      <td>GLUTEN FREE BREAD</td>\n      <td>THIS BREAD MIX IS THE CLOSEST THING TO REGULAR...</td>\n    </tr>\n    <tr>\n      <th>29997</th>\n      <td>29998</td>\n      <td>B000DZFMEQ</td>\n      <td>A2LQTTTXBLFFAO</td>\n      <td>lovereading \"Gin\"</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1250208000</td>\n      <td>Pamela's bread mix</td>\n      <td>Delicious and easy to make. An excellent bread...</td>\n    </tr>\n    <tr>\n      <th>29998</th>\n      <td>29999</td>\n      <td>B000DZFMEQ</td>\n      <td>A2A0UL2OFEIPH4</td>\n      <td>Jo \"Jo\"</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1248048000</td>\n      <td>Great bread!</td>\n      <td>I bought this mix for my daughter's boyfriend,...</td>\n    </tr>\n    <tr>\n      <th>29999</th>\n      <td>30000</td>\n      <td>B000DZFMEQ</td>\n      <td>A32HT263MGTCQD</td>\n      <td>Keith Roys</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1247184000</td>\n      <td>Great price on a great gluten-free product!</td>\n      <td>We have paid over a package in health food sto...</td>\n    </tr>\n  </tbody>\n</table>\n<p>30000 rows × 10 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "training_data, testing_data = train_test_split(subdf, test_size = 0.25, random_state = 42)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-22T11:59:39.113325Z",
          "iopub.execute_input": "2024-04-22T11:59:39.113712Z",
          "iopub.status.idle": "2024-04-22T12:00:19.758214Z",
          "shell.execute_reply.started": "2024-04-22T11:59:39.113665Z",
          "shell.execute_reply": "2024-04-22T12:00:19.756671Z"
        },
        "trusted": true,
        "id": "8YodG8xyvBbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DatasetTokenizer(Dataset):\n",
        "    def __init__(self, tokenizer, dataframe, max_length):\n",
        "        if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
        "        if tokenizer.pad_token is None: tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def ReturnStatement(self, input_ids, attention_mask):\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': input_ids\n",
        "        }\n",
        "\n",
        "    def flattenvectors(self, lst, inputs):\n",
        "        input_ids = None\n",
        "        attention_mask = None\n",
        "        labels = None\n",
        "        for item in lst:\n",
        "            if item == 'input_ids' or item == 'labels':\n",
        "                item = 'input_ids'\n",
        "                input_ids = inputs[item].flatten()\n",
        "                labels = inputs[item].flatten()\n",
        "            else:\n",
        "                attention_mask = inputs[item].flatten()\n",
        "        return input_ids, attention_mask, labels\n",
        "\n",
        "    def _encode(self, review, summary):\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            review, summary, add_special_tokens=True,\n",
        "            max_length=self.max_length, padding='max_length', truncation = True, return_tensors='pt'\n",
        "        )\n",
        "        return inputs\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        review = str(self.data.iloc[index]['Text'])\n",
        "        summary = str(self.data.iloc[index]['Summary'])\n",
        "        inputs = self._encode(review, summary)\n",
        "        input_ids, attention_mask, labels = self.flattenvectors(['input_ids', 'attention_mask', 'labels'], inputs)\n",
        "        return self.ReturnStatement(input_ids, attention_mask)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-22T12:07:54.154186Z",
          "iopub.execute_input": "2024-04-22T12:07:54.154603Z",
          "iopub.status.idle": "2024-04-22T12:07:54.169284Z",
          "shell.execute_reply.started": "2024-04-22T12:07:54.154559Z",
          "shell.execute_reply": "2024-04-22T12:07:54.168255Z"
        },
        "trusted": true,
        "id": "hi6loOejvBbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "def set_training_arguments():\n",
        "    _output_dir='content/results'\n",
        "    _num_train_epochs=3\n",
        "    _per_device_train_batch_size=4\n",
        "    _per_device_eval_batch_size=8\n",
        "    _warmup_steps=500\n",
        "    _weight_decay=0.01\n",
        "    _logging_dir='content/logs'\n",
        "    _evaluation_strategy='epoch'\n",
        "    return TrainingArguments(\n",
        "        output_dir=_output_dir,\n",
        "        num_train_epochs=_num_train_epochs,\n",
        "        per_device_train_batch_size=_per_device_train_batch_size,\n",
        "        per_device_eval_batch_size=_per_device_eval_batch_size,\n",
        "        warmup_steps=_warmup_steps,\n",
        "        weight_decay=_weight_decay,\n",
        "        logging_dir=_logging_dir,\n",
        "        evaluation_strategy=_evaluation_strategy\n",
        "    )\n",
        "\n",
        "def create_trainer(model, train_dataset, test_dataset, training_args):\n",
        "    return Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset\n",
        "    )\n",
        "\n",
        "def train_model(trainer):\n",
        "    trainer.train(\"/kaggle/working/content/results/checkpoint-7000\")\n",
        "\n",
        "def train_model_with_parameters(model, train_dataset, test_dataset):\n",
        "    training_args = set_training_arguments()\n",
        "    trainer = create_trainer(model, train_dataset, test_dataset, training_args)\n",
        "    train_model(trainer)\n",
        "\n",
        "# Assuming DatasetTokenizer is a custom class for tokenizing datasets\n",
        "train_dataset = DatasetTokenizer(tokenizer, training_data, max_length=512)\n",
        "test_dataset = DatasetTokenizer(tokenizer, testing_data, max_length=512)\n",
        "train_model_with_parameters(model, train_dataset, test_dataset)"
      ],
      "metadata": {
        "trusted": true,
        "id": "xL5FIAbrvBbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"model\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-22T12:00:22.707461Z",
          "iopub.status.idle": "2024-04-22T12:00:22.708006Z",
          "shell.execute_reply.started": "2024-04-22T12:00:22.707725Z",
          "shell.execute_reply": "2024-04-22T12:00:22.707747Z"
        },
        "trusted": true,
        "id": "BAknVGSFvBbI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}